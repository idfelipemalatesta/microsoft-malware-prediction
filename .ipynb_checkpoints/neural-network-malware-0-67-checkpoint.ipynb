{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Neural Network - Statistical Encoding - Microsoft Malware\n",
    "There aren't any examples of using a neural network to model Microsoft Malware, so I thought I'd post one. Also in this kernel, I show statistical one-hot-encoding where only boolean variables that are independently statistically significant are created.\n",
    "\n",
    "# Load Train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8921483 rows of TRAIN.CSV!\n",
      "Only using 2000000 rows to train and validate\n"
     ]
    }
   ],
   "source": [
    "# IMPORT LIBRARIES\n",
    "import pandas as pd, numpy as np, os, gc\n",
    "\n",
    "# LOAD AND FREQUENCY-ENCODE\n",
    "FE = ['EngineVersion','AppVersion','AvSigVersion','Census_OSVersion']\n",
    "# LOAD AND ONE-HOT-ENCODE\n",
    "OHE = [ 'RtpStateBitfield','IsSxsPassiveMode','DefaultBrowsersIdentifier',\n",
    "        'AVProductStatesIdentifier','AVProductsInstalled', 'AVProductsEnabled',\n",
    "        'CountryIdentifier', 'CityIdentifier', \n",
    "        'GeoNameIdentifier', 'LocaleEnglishNameIdentifier',\n",
    "        'Processor', 'OsBuild', 'OsSuite',\n",
    "        'SmartScreen','Census_MDC2FormFactor',\n",
    "        'Census_OEMNameIdentifier', \n",
    "        'Census_ProcessorCoreCount',\n",
    "        'Census_ProcessorModelIdentifier', \n",
    "        'Census_PrimaryDiskTotalCapacity', 'Census_PrimaryDiskTypeName',\n",
    "        'Census_HasOpticalDiskDrive',\n",
    "        'Census_TotalPhysicalRAM', 'Census_ChassisTypeName',\n",
    "        'Census_InternalPrimaryDiagonalDisplaySizeInInches',\n",
    "        'Census_InternalPrimaryDisplayResolutionHorizontal',\n",
    "        'Census_InternalPrimaryDisplayResolutionVertical',\n",
    "        'Census_PowerPlatformRoleName', 'Census_InternalBatteryType',\n",
    "        'Census_InternalBatteryNumberOfCharges',\n",
    "        'Census_OSEdition', 'Census_OSInstallLanguageIdentifier',\n",
    "        'Census_GenuineStateName','Census_ActivationChannel',\n",
    "        'Census_FirmwareManufacturerIdentifier',\n",
    "        'Census_IsTouchEnabled', 'Census_IsPenCapable',\n",
    "        'Census_IsAlwaysOnAlwaysConnectedCapable', 'Wdft_IsGamer',\n",
    "        'Wdft_RegionIdentifier']\n",
    "\n",
    "# LOAD ALL AS CATEGORIES\n",
    "dtypes = {}\n",
    "for x in FE+OHE: dtypes[x] = 'category'\n",
    "dtypes['MachineIdentifier'] = 'str'\n",
    "dtypes['HasDetections'] = 'int8'\n",
    "\n",
    "# LOAD CSV FILE\n",
    "df_train = pd.read_csv('../input/train.csv', usecols=dtypes.keys(), dtype=dtypes)\n",
    "print ('Loaded',len(df_train),'rows of TRAIN.CSV!')\n",
    "\n",
    "# DOWNSAMPLE\n",
    "sm = 2000000\n",
    "df_train = df_train.sample(sm)\n",
    "print ('Only using',sm,'rows to train and validate')\n",
    "x=gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f7a58a55b1617fefb9e9577546e11aa6252e4935"
   },
   "source": [
    "# Statistically Encode Variables\n",
    "All the variables in the Python variable list `FE` will get frequency encoded and all the variables in list `OHE` will get statistically one-hot-encoded. In total, forty-three variables are imported from the training csv while thirty-nine were ignored.\n",
    "  \n",
    "Among all our category variables, there are a combined 211,562 values! So we can't one-hot-encode all. (Note that this is without Census_OEMModelIdentifier's 175,366 or Census_SystemVolumeTotalCapacity's 536,849) We will use a trick from statistics. First we'll assume we have a random sample. (Which we don't actually have, but let's pretend.) Then for each value, we will test the following hypotheses   \n",
    "\n",
    " $$H_0: \\text{Prob(HasDetections=1 given value is present)} = 0.5 $$ \n",
    " $$H_A: \\text{Prob(HasDetections=1 given value is present)} \\ne 0.5$$  \n",
    "    \n",
    "The test statistic z-value equals \\\\( \\hat{p} \\\\), the observed HasDetections rate given value is present, minus 0.5 divided by the standard deviation of \\\\( \\hat{p} \\\\). The Central Limit Theorem tells us\n",
    "\n",
    "$$\\text{z-value} = \\frac{\\hat{p}-0.5}{SD(\\hat{p})} = 2 (\\hat{p} - 0.5)\\sqrt{n} $$\n",
    "\n",
    "where \\\\(n\\\\) is the number of occurences of the value. If the absolute value of \\\\(z\\\\) is greater than 2.0, we are 95% confident that Prob(HasDetections=1 given value is present) is not equal 0.5 and we will include a boolean for this value in our model. Actually, we'll use a \\\\(z\\\\) threshold of 5.0 and require \\\\( 10^{-7}n>0.005 \\\\). This adds 350 new boolean variables (instead of naively one-hot-encoding 211,562!).  \n",
    "  \n",
    " ## Example - Census_FirmwareManufacturerIdentifier\n",
    "In the plots below, the dotted lines use the right y-axis and solid lines/bars use the left. The top plot below shows 20 values of variable `Census_FirmwareManufacturerIdentifier`. Notice that I consider NAN a value. Each of these values contains over 0.5% of the data. And all the variables together contain 97% of the data. Value=93 has a HasDetections rate of 52.5% while value=803 has a HasDetections rate of 35.4%. Their z-values are \\\\(22.2 = 2\\times(0.5253-0.5)\\times\\sqrt{192481} \\text{  }\\\\)  and \\\\(-71.3 = 2\\times(0.3535-0.5)\\times\\sqrt{59145}\\text{  }\\\\)   respectively! The probability that value=93 and value=803 have a HasDetections rate of 50% and what we are observing is due to chance is close to nothing. Additionally from the bottom plot, you see that these two values have consistently been high and low throughout all of the year 2018. We can trust that this trend will continue into the test set's October and November computers.  \n",
    "   \n",
    "![image](http://playagricola.com/Kaggle/Firm13019.png)\n",
    "\n",
    "## Python Code\n",
    "To see the Python encoding functions, click 'see code' to the right.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "1fadf38ea607fb94432c728a140c53f0f66139ca"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# CHECK FOR NAN\n",
    "def nan_check(x):\n",
    "    if isinstance(x,float):\n",
    "        if math.isnan(x):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# FREQUENCY ENCODING\n",
    "def encode_FE(df,col,verbose=1):\n",
    "    d = df[col].value_counts(dropna=False)\n",
    "    n = col+\"_FE\"\n",
    "    df[n] = df[col].map(d)/d.max()\n",
    "    if verbose==1:\n",
    "        print('FE encoded',col)\n",
    "    return [n]\n",
    "\n",
    "# ONE-HOT-ENCODE ALL CATEGORY VALUES THAT COMPRISE MORE THAN\n",
    "# \"FILTER\" PERCENT OF TOTAL DATA AND HAS SIGNIFICANCE GREATER THAN \"ZVALUE\"\n",
    "def encode_OHE(df, col, filter, zvalue, tar='HasDetections', m=0.5, verbose=1):\n",
    "    cv = df[col].value_counts(dropna=False)\n",
    "    cvd = cv.to_dict()\n",
    "    vals = len(cv)\n",
    "    th = filter * len(df)\n",
    "    sd = zvalue * 0.5/ math.sqrt(th)\n",
    "    #print(sd)\n",
    "    n = []; ct = 0; d = {}\n",
    "    for x in cv.index:\n",
    "        try:\n",
    "            if cv[x]<th: break\n",
    "            sd = zvalue * 0.5/ math.sqrt(cv[x])\n",
    "        except:\n",
    "            if cvd[x]<th: break\n",
    "            sd = zvalue * 0.5/ math.sqrt(cvd[x])\n",
    "        if nan_check(x): r = df[df[col].isna()][tar].mean()\n",
    "        else: r = df[df[col]==x][tar].mean()\n",
    "        if abs(r-m)>sd:\n",
    "            nm = col+'_BE_'+str(x)\n",
    "            if nan_check(x): df[nm] = (df[col].isna()).astype('int8')\n",
    "            else: df[nm] = (df[col]==x).astype('int8')\n",
    "            n.append(nm)\n",
    "            d[x] = 1\n",
    "        ct += 1\n",
    "        if (ct+1)>=vals: break\n",
    "    if verbose==1:\n",
    "        print('OHE encoded',col,'- Created',len(d),'booleans')\n",
    "    return [n,d]\n",
    "\n",
    "# ONE-HOT-ENCODING from dictionary\n",
    "def encode_OHE_test(df,col,dt):\n",
    "    n = []\n",
    "    for x in dt: \n",
    "        n += encode_BE(df,col,x)\n",
    "    return n\n",
    "\n",
    "# BOOLEAN ENCODING\n",
    "def encode_BE(df,col,val):\n",
    "    n = col+\"_BE_\"+str(val)\n",
    "    if nan_check(val):\n",
    "        df[n] = df[col].isna()\n",
    "    else:\n",
    "        df[n] = df[col]==val\n",
    "    df[n] = df[n].astype('int8')\n",
    "    return [n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "afa1ab97768b80133161a00f0037f652f29e648b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE encoded EngineVersion\n",
      "FE encoded AppVersion\n",
      "FE encoded AvSigVersion\n",
      "FE encoded Census_OSVersion\n",
      "OHE encoded RtpStateBitfield - Created 2 booleans\n",
      "OHE encoded IsSxsPassiveMode - Created 1 booleans\n",
      "OHE encoded DefaultBrowsersIdentifier - Created 1 booleans\n",
      "OHE encoded AVProductStatesIdentifier - Created 9 booleans\n",
      "OHE encoded AVProductsInstalled - Created 3 booleans\n",
      "OHE encoded AVProductsEnabled - Created 2 booleans\n",
      "OHE encoded CountryIdentifier - Created 40 booleans\n",
      "OHE encoded CityIdentifier - Created 11 booleans\n",
      "OHE encoded GeoNameIdentifier - Created 32 booleans\n",
      "OHE encoded LocaleEnglishNameIdentifier - Created 26 booleans\n",
      "OHE encoded Processor - Created 2 booleans\n",
      "OHE encoded OsBuild - Created 6 booleans\n",
      "OHE encoded OsSuite - Created 2 booleans\n",
      "OHE encoded SmartScreen - Created 4 booleans\n",
      "OHE encoded Census_MDC2FormFactor - Created 5 booleans\n",
      "OHE encoded Census_OEMNameIdentifier - Created 20 booleans\n",
      "OHE encoded Census_ProcessorCoreCount - Created 6 booleans\n",
      "OHE encoded Census_ProcessorModelIdentifier - Created 27 booleans\n",
      "OHE encoded Census_PrimaryDiskTotalCapacity - Created 14 booleans\n",
      "OHE encoded Census_PrimaryDiskTypeName - Created 3 booleans\n",
      "OHE encoded Census_HasOpticalDiskDrive - Created 1 booleans\n",
      "OHE encoded Census_TotalPhysicalRAM - Created 9 booleans\n",
      "OHE encoded Census_ChassisTypeName - Created 10 booleans\n",
      "OHE encoded Census_InternalPrimaryDiagonalDisplaySizeInInches - Created 21 booleans\n",
      "OHE encoded Census_InternalPrimaryDisplayResolutionHorizontal - Created 4 booleans\n",
      "OHE encoded Census_InternalPrimaryDisplayResolutionVertical - Created 6 booleans\n",
      "OHE encoded Census_PowerPlatformRoleName - Created 3 booleans\n",
      "OHE encoded Census_InternalBatteryType - Created 4 booleans\n",
      "OHE encoded Census_InternalBatteryNumberOfCharges - Created 4 booleans\n",
      "OHE encoded Census_OSEdition - Created 4 booleans\n",
      "OHE encoded Census_OSInstallLanguageIdentifier - Created 18 booleans\n",
      "OHE encoded Census_GenuineStateName - Created 2 booleans\n",
      "OHE encoded Census_ActivationChannel - Created 4 booleans\n",
      "OHE encoded Census_FirmwareManufacturerIdentifier - Created 13 booleans\n",
      "OHE encoded Census_IsTouchEnabled - Created 1 booleans\n",
      "OHE encoded Census_IsPenCapable - Created 0 booleans\n",
      "OHE encoded Census_IsAlwaysOnAlwaysConnectedCapable - Created 2 booleans\n",
      "OHE encoded Wdft_IsGamer - Created 2 booleans\n",
      "OHE encoded Wdft_RegionIdentifier - Created 12 booleans\n",
      "Encoded 340 new variables\n",
      "Removed original 43 variables\n"
     ]
    }
   ],
   "source": [
    "cols = []; dd = []\n",
    "\n",
    "# ENCODE NEW\n",
    "for x in FE:\n",
    "    cols += encode_FE(df_train,x)\n",
    "for x in OHE:\n",
    "    tmp = encode_OHE(df_train,x,0.005,5)\n",
    "    cols += tmp[0]; dd.append(tmp[1])\n",
    "print('Encoded',len(cols),'new variables')\n",
    "\n",
    "# REMOVE OLD\n",
    "for x in FE+OHE:\n",
    "    del df_train[x]\n",
    "print('Removed original',len(FE+OHE),'variables')\n",
    "x = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "30aa82afc92de7e295d11429636b5c50163c4e8c"
   },
   "source": [
    "## Example - Census_OEMModelIdentifier\n",
    "Below is variable `Census_OEMModelIdentifier`. Observe how NAN is treated like a category value and that it has consistently had the lowest HasDetections rate all of year 2018. Also notice how value=245824 has consistently been high. Finally note that value=188345 and 248045 are high and low respectively in August and September but earlier in the year their positions were reversed! What will their positions be in the test set's October and November computers??  \n",
    "  \n",
    "![image](http://playagricola.com/Kaggle/OEM13019.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c99423ce7d2c30395ddddaf39ec7d56d87defa17"
   },
   "source": [
    "# Build and Train Network\n",
    "We will a build a 3 layer fully connected network with 100 neurons on each hidden layer. We will use ReLU activation, Batch Normalization, 40% Dropout, Adam Optimizer, and Decaying Learning Rate. Unfortunately we don't have an AUC loss function, so we will use Cross Entrophy instead. After each epoch, we will call a custom Keras callback to display the current AUC and continually save the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "dc3ee8729926910d48e9543e73e7f29791dc399d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import callbacks\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class printAUC(callbacks.Callback):\n",
    "    def __init__(self, X_train, y_train):\n",
    "        super(printAUC, self).__init__()\n",
    "        self.bestAUC = 0\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pred = self.model.predict(np.array(self.X_train))\n",
    "        auc = roc_auc_score(self.y_train, pred)\n",
    "        print(\"Train AUC: \" + str(auc))\n",
    "        pred = self.model.predict(self.validation_data[0])\n",
    "        auc = roc_auc_score(self.validation_data[1], pred)\n",
    "        print (\"Validation AUC: \" + str(auc))\n",
    "        if (self.bestAUC < auc) :\n",
    "            self.bestAUC = auc\n",
    "            self.model.save(\"bestNet.h5\", overwrite=True)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "611fc3d8e0d92288a6c1d558a9ebaed10cef97e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000000 samples, validate on 1000000 samples\n",
      "Epoch 1/20\n",
      " - 128s - loss: 0.6444 - acc: 0.6206 - val_loss: 0.6339 - val_acc: 0.6273\n",
      "Train AUC: 0.6933346462001464\n",
      "Validation AUC: 0.6912612506941198\n",
      "Epoch 2/20\n",
      " - 127s - loss: 0.6387 - acc: 0.6280 - val_loss: 0.6282 - val_acc: 0.6362\n",
      "Train AUC: 0.6982506064751803\n",
      "Validation AUC: 0.6956000576475689\n",
      "Epoch 3/20\n",
      " - 131s - loss: 0.6368 - acc: 0.6305 - val_loss: 0.6286 - val_acc: 0.6370\n",
      "Train AUC: 0.7007412986372941\n",
      "Validation AUC: 0.6973941619062344\n",
      "Epoch 4/20\n",
      " - 126s - loss: 0.6358 - acc: 0.6316 - val_loss: 0.6269 - val_acc: 0.6375\n",
      "Train AUC: 0.7016034446417853\n",
      "Validation AUC: 0.6978407744028299\n",
      "Epoch 5/20\n",
      " - 126s - loss: 0.6346 - acc: 0.6330 - val_loss: 0.6258 - val_acc: 0.6398\n",
      "Train AUC: 0.7031193933359536\n",
      "Validation AUC: 0.6991105853011363\n",
      "Epoch 6/20\n",
      " - 126s - loss: 0.6341 - acc: 0.6333 - val_loss: 0.6271 - val_acc: 0.6392\n",
      "Train AUC: 0.7038570320998554\n",
      "Validation AUC: 0.6996470556494905\n",
      "Epoch 7/20\n",
      " - 127s - loss: 0.6333 - acc: 0.6342 - val_loss: 0.6266 - val_acc: 0.6386\n",
      "Train AUC: 0.7045994300426082\n",
      "Validation AUC: 0.7000486435451285\n",
      "Epoch 8/20\n",
      " - 126s - loss: 0.6327 - acc: 0.6344 - val_loss: 0.6285 - val_acc: 0.6402\n",
      "Train AUC: 0.7055087691573714\n",
      "Validation AUC: 0.7006234463009472\n",
      "Epoch 9/20\n",
      " - 127s - loss: 0.6324 - acc: 0.6349 - val_loss: 0.6255 - val_acc: 0.6411\n",
      "Train AUC: 0.7061243855261722\n",
      "Validation AUC: 0.7009121400139172\n",
      "Epoch 10/20\n",
      " - 128s - loss: 0.6319 - acc: 0.6356 - val_loss: 0.6307 - val_acc: 0.6405\n",
      "Train AUC: 0.7065540292175616\n",
      "Validation AUC: 0.7010786840586227\n",
      "Epoch 11/20\n",
      " - 126s - loss: 0.6314 - acc: 0.6366 - val_loss: 0.6238 - val_acc: 0.6414\n",
      "Train AUC: 0.7072878977532046\n",
      "Validation AUC: 0.7015355407856272\n",
      "Epoch 12/20\n",
      " - 125s - loss: 0.6312 - acc: 0.6370 - val_loss: 0.6242 - val_acc: 0.6412\n",
      "Train AUC: 0.7079650916308952\n",
      "Validation AUC: 0.7020796169582244\n",
      "Epoch 13/20\n",
      " - 126s - loss: 0.6310 - acc: 0.6369 - val_loss: 0.6248 - val_acc: 0.6406\n",
      "Train AUC: 0.7082480756844242\n",
      "Validation AUC: 0.7018906517431152\n",
      "Epoch 14/20\n",
      " - 127s - loss: 0.6303 - acc: 0.6374 - val_loss: 0.6245 - val_acc: 0.6404\n",
      "Train AUC: 0.7089586725801257\n",
      "Validation AUC: 0.7024551821405017\n",
      "Epoch 15/20\n",
      " - 127s - loss: 0.6301 - acc: 0.6377 - val_loss: 0.6259 - val_acc: 0.6423\n",
      "Train AUC: 0.7090732589227717\n",
      "Validation AUC: 0.7024799888668641\n",
      "Epoch 16/20\n",
      " - 126s - loss: 0.6297 - acc: 0.6378 - val_loss: 0.6231 - val_acc: 0.6425\n",
      "Train AUC: 0.7098057367795816\n",
      "Validation AUC: 0.7027273728479257\n",
      "Epoch 17/20\n",
      " - 126s - loss: 0.6294 - acc: 0.6388 - val_loss: 0.6238 - val_acc: 0.6419\n",
      "Train AUC: 0.7093010620392429\n",
      "Validation AUC: 0.7023760449935433\n",
      "Epoch 18/20\n",
      " - 127s - loss: 0.6292 - acc: 0.6389 - val_loss: 0.6258 - val_acc: 0.6426\n",
      "Train AUC: 0.7099419919152088\n",
      "Validation AUC: 0.7028040761968868\n",
      "Epoch 19/20\n",
      " - 129s - loss: 0.6290 - acc: 0.6391 - val_loss: 0.6244 - val_acc: 0.6421\n",
      "Train AUC: 0.7105092539150418\n",
      "Validation AUC: 0.7030129516163394\n",
      "Epoch 20/20\n",
      " - 128s - loss: 0.6286 - acc: 0.6391 - val_loss: 0.6254 - val_acc: 0.6415\n",
      "Train AUC: 0.7104918925106409\n",
      "Validation AUC: 0.7030240908474825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efd17083908>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#SPLIT TRAIN AND VALIDATION SET\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    df_train[cols], df_train['HasDetections'], test_size = 0.5)\n",
    "\n",
    "# BUILD MODEL\n",
    "model = Sequential()\n",
    "model.add(Dense(100,input_dim=len(cols)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(100))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=Adam(lr=0.01), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "annealer = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** x)\n",
    "\n",
    "# TRAIN MODEL\n",
    "model.fit(X_train,Y_train, batch_size=32, epochs = 20, callbacks=[annealer,\n",
    "          printAUC(X_train, Y_train)], validation_data = (X_val,Y_val), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4dbc958dd56c5af4d74bdeded9696ff244246f02"
   },
   "source": [
    "# Predict Test and Submit to Kaggle\n",
    "Even after deleting the training data, our network still needs lot of our available RAM, we'll need to load in test.csv by chunks and predict by chunks. Click 'see code' button to see how this is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "96e7fbf730e30707f04e4b192c839c820af5dee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000000 rows of TEST.CSV!\n",
      "  encoded and predicted part 1\n",
      "Loaded 2000000 rows of TEST.CSV!\n",
      "  encoded and predicted part 2\n",
      "Loaded 2000000 rows of TEST.CSV!\n",
      "  encoded and predicted part 3\n",
      "Loaded 1853253 rows of TEST.CSV!\n",
      "  encoded and predicted part 4\n"
     ]
    }
   ],
   "source": [
    "del df_train\n",
    "del X_train, X_val, Y_train, Y_val\n",
    "x = gc.collect()\n",
    "\n",
    "# LOAD BEST SAVED NET\n",
    "from keras.models import load_model\n",
    "model = load_model('bestNet.h5')\n",
    "\n",
    "pred = np.zeros((7853253,1))\n",
    "id = 1\n",
    "chunksize = 2000000\n",
    "for df_test in pd.read_csv('../input/test.csv', \n",
    "            chunksize = chunksize, usecols=list(dtypes.keys())[0:-1], dtype=dtypes):\n",
    "    print ('Loaded',len(df_test),'rows of TEST.CSV!')\n",
    "    # ENCODE TEST\n",
    "    cols = []\n",
    "    for x in FE:\n",
    "        cols += encode_FE(df_test,x,verbose=0)\n",
    "    for x in range(len(OHE)):\n",
    "        cols += encode_OHE_test(df_test,OHE[x],dd[x])\n",
    "    # PREDICT TEST\n",
    "    end = (id)*chunksize\n",
    "    if end>7853253: end = 7853253\n",
    "    pred[(id-1)*chunksize:end] = model.predict(df_test[cols])\n",
    "    print('  encoded and predicted part',id)\n",
    "    id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "c50d62226fb92795556adcf192b685c3e496b14e"
   },
   "outputs": [],
   "source": [
    "# SUBMIT TO KAGGLE\n",
    "df_test = pd.read_csv('../input/test.csv', usecols=['MachineIdentifier'])\n",
    "df_test['HasDetections'] = pred\n",
    "df_test.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f873a4954f0f70419c331a93dcc04c2f2fb97b04"
   },
   "source": [
    "![image](http://playagricola.com/Kaggle/NN13019.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e161d9a3a1c67d29ac1e991235b6b4ed7c12f19e"
   },
   "source": [
    "# Conclusion\n",
    "In this kernel, we saw how to build and train a neural network with Keras. We also saw how to statistically one-hot-encode categorical variables. Our validation AUC was 0.703 and our LB was 0.671. So it appears that we are not time generalizing enough to the test set. Furthermore, other users are getting higher CV scores, so we should be able to improve our AUC by adding more variables and tuning our network more. \n",
    "\n",
    "If anyone forks this kernel and improves it's AUC, let me know. All comments and suggestions are welcome. Thanks for reading."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
